"""Mixture model for matrix completion"""
from typing import Tuple
import numpy as np
from scipy.special import logsumexp
from common import GaussianMixture


def estep(X: np.ndarray, mixture: GaussianMixture) -> Tuple[np.ndarray, float]:
    """E-step: Softly assigns each datapoint to a gaussian component

    Args:
        X: (n, d) array holding the data, with incomplete entries (set to 0)
        mixture: the current gaussian mixture

    Returns:
        np.ndarray: (n, K) array holding the soft counts
            for all components for all examples
        float: log-likelihood of the assignment

    """
    var = mixture.var
    p = mixture.p
    n, _ = X.shape
    K, _ = mixture.mu.shape

    likelihood = np.zeros((n, K))

    for i in range(n):
        ix = np.nonzero(X[i])[0]
        x = X[i, ix]
        mu = mixture.mu[:, ix]
        d = x.shape[0]
        for j in range(K):
            # Multivariate Gaussian, N(x; mu, var)
            sigma = var[j] * np.identity(d)
            det = np.power(var[j], d)
            numerator = (-1 / 2) * ((x - mu[j]).T.dot(np.linalg.inv(sigma))).dot((x - mu[j]))
            denominator = np.power(2 * np.pi, d / 2) * np.sqrt(det)

            gaussian = np.exp(numerator) / denominator

            # P(x generated by j | theta) = mixing proportion * N(x; mu, var)
            likelihood[i, j] = np.log(p[j] + 1e-16) + np.log(gaussian)

    # posterior probability, P(j|i)
    log_post = likelihood - logsumexp(likelihood, axis=1, keepdims=True)
    post = np.exp(log_post)

    # log-likelihood

    #LL = logsumexp(likelihood, axis=1).sum()
    print(likelihood.shape)
    print(likelihood.min)
    print(likelihood.max)
    LL = np.log(np.exp(likelihood).sum(axis=1)).sum()

    return post, LL


def mstep(X: np.ndarray, post: np.ndarray, mixture: GaussianMixture,
          min_variance: float = .25) -> GaussianMixture:
    """M-step: Updates the gaussian mixture by maximizing the log-likelihood
    of the weighted dataset

    Args:
        X: (n, d) array holding the data, with incomplete entries (set to 0)
        post: (n, K) array holding the soft counts
            for all components for all examples
        mixture: the current gaussian mixture
        min_variance: the minimum variance for each gaussian

    Returns:
        GaussianMixture: the new gaussian mixture
    """
    n, _ = post.shape
    K, d = mixture.mu.shape

    mu = np.zeros((K, d))
    var = np.zeros(K)
    p = np.zeros(K)
    indicator = np.heaviside(X, 0)

    for j in range(K):
        # find the mixture model given the derivative of the fixed likelihood function

        # update only mu for features in d where at least one full point supports the mean
        #   since we are dealing with incomplete data, we might have a case where most of the points in cluster 𝑗 are
        #   missing the 𝑖-th coordinate
        mu_denominator = np.sum(post[:, j].reshape(n, 1) * indicator, axis=0)
        mu_numerator = np.sum(post[:, j].reshape(n, 1) * indicator * X, axis=0)
        ix = np.where(mu_denominator >= 1)

        mu[j, ix] = mu_numerator[ix] / mu_denominator[ix]

        var_numerator = np.sum(post[:, j].reshape(n, 1) * np.square((X - mu[j]) * indicator))
        var_denominator = np.sum(np.sum(indicator, axis=1) * post[:, j])

        var[j] = max(var_numerator / var_denominator, min_variance)

        p[j] = (1 / n) * np.sum(post[:, j])

    return GaussianMixture(mu, var, p)


def run(X: np.ndarray, mixture: GaussianMixture,
        post: np.ndarray) -> Tuple[GaussianMixture, np.ndarray, float]:
    """Runs the mixture model

    Args:
        X: (n, d) array holding the data
        post: (n, K) array holding the soft counts
            for all components for all examples

    Returns:
        GaussianMixture: the new gaussian mixture
        np.ndarray: (n, K) array holding the soft counts
            for all components for all examples
        float: log-likelihood of the current assignment
    """
    LL_old = None
    LL_new = None

    while (LL_old is None or (LL_new - LL_old) >= (np.abs(LL_new) * 1e-6)):
        LL_old = LL_new
        post, LL_new = estep(X, mixture)
        mixture = mstep(X, post, mixture)

    return mixture, post, LL_new


def fill_matrix(X: np.ndarray, mixture: GaussianMixture) -> np.ndarray:
    """Fills an incomplete matrix according to a mixture model

    Args:
        X: (n, d) array of incomplete data (incomplete entries =0)
        mixture: a mixture of gaussians

    Returns
        np.ndarray: a (n, d) array with completed data
    """
    raise NotImplementedError


if __name__ == '__main__':
    import common

    X = np.loadtxt("toy_data.txt")

    mixture, post = common.init(X, K=2, seed=0)

    estep(X, mixture)